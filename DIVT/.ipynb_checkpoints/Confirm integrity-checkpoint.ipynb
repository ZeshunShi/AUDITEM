{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipfshttpclient'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-75ad9056ea0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mipfshttpclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipfshttpclient'"
     ]
    }
   ],
   "source": [
    "import ipfshttpclient\n",
    "import requests\n",
    "import json \n",
    "import fiona \n",
    "import geopandas\n",
    "import pandas as pd\n",
    "client = ipfshttpclient.connect()\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import Crypto\n",
    "from Crypto.Cipher import AES\n",
    "from base64 import b64encode, b64decode\n",
    "import numpy as np\n",
    "import time \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"DWH-User\"\n",
    "organisationDB_Name = \"SQlite-file\"\n",
    "username = \"DWH-User\"\n",
    "organisation = \"Org1\"\n",
    "key = \"secret-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add user and database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = add_user(username, organisation)\n",
    "tables_filtered = addDB(DB_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confirm the integrity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Confirm_integrity(DB_Name, tables_filtered, organisation, username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To load the dataframe\n",
    "\n",
    "def get_tables(db):  #Get a list of the tables that are inside the database\n",
    "    cursor = db.cursor()\n",
    "    tables = list()\n",
    "    for name in cursor.execute('SELECT * FROM gpkg_contents;'):\n",
    "        tables.append(name[0])   \n",
    "    cursor.close()\n",
    "    return(tables)\n",
    "\n",
    "def filter_tables(db, tables): #Filter for usefull tables, return a list of the useful tables. \n",
    "    tables_filterd = list()\n",
    "    cursor = db.cursor()\n",
    "    for table in tables:\n",
    "        colNames = list() \n",
    "        for name in cursor.execute('PRAGMA table_info('+ table +');'):\n",
    "            colNames.append(name[1])\n",
    "        if \"metahistoryinsbatchid\" in colNames: #We only take tables that have the list metahistoryinsbatchid\n",
    "            tables_filterd.append(table)        \n",
    "    cursor.close() \n",
    "    return(tables_filterd) \n",
    "\n",
    "\n",
    "def get_amount_batches(tables_filtered, db):  #Get the numer of batched in the first col (BETTER TO LOOK AT AL DB\"s AND SELECT the most)\n",
    "    df = pd.read_sql_query(\"SELECT * from \" + tables_filtered[0], db)\n",
    "    return(df[\"metahistoryinsbatchid\"].unique())\n",
    "    \n",
    "\n",
    "def addDB(File_name): #Add DB To the system \n",
    "    db = sqlite3.connect(File_name)\n",
    "    tables = get_tables(db)\n",
    "    tables_filtered = filter_tables(db, tables)\n",
    "    print(\"There are: \" + str(len(tables_filtered)) + \" tables\")\n",
    "    print(\"Containing: \"+ str(len(get_amount_batches(tables_filtered, db))) + \" batches\")\n",
    "    db.close()\n",
    "    return(tables_filtered)\n",
    "\n",
    "#To add the dataframe\n",
    "\n",
    "def add_user(username, organization): #add user using the API in hyperledger, returns the token needed to add transaction tot the chain. \n",
    "    URL =\"http://localhost:4000/users\"\n",
    "    PARAMS = {'username':username, \n",
    "              'orgName': organization} \n",
    "    r = requests.post(url = URL, json = PARAMS)   \n",
    "    token = json.loads(r.text)['token'] \n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to check the integrity, input is the database, what are the usefull tables and what organisation we are. It uses the identification attributes to retrieve the verification hash, \n",
    "#checks the verification hash with the verification hash on the blockchain. Only uses the verification attributes when the identificaion hash is wrong. \n",
    "def Confirm_integrity(file_name, tables_filtered, organisation, username):\n",
    "    db = sqlite3.connect(file_name)\n",
    "    final_reponds = 0\n",
    "    for table_name in tables_filtered:\n",
    "        table = get_table(table_name, db)\n",
    "        for batch_number in table[\"metahistoryinsbatchid\"].unique():\n",
    "            subset = get_batch(table, batch_number)\n",
    "            s_v, h_v = verification_attributes(subset, organisation, table_name)\n",
    "            h_i_, h_v_= get_from_blockchain(organisation, table_name, batch_number)\n",
    "            print(table_name + \" \" + str(batch_number) + \":\")\n",
    "            responds = verify_1(h_v, h_v_)\n",
    "            if (responds == 1):\n",
    "                final_reponds = 1\n",
    "                verify_2(h_i_, s_v, key)\n",
    "    if (final_reponds == 0):\n",
    "        print('\\x1b[1;32m'+'Integrity of: ' + file_name + ' confirmed' + '\\x1b[0m')\n",
    "        responds_id = sent_certificate(organisation, file_name, username)\n",
    "        print(\"ID = \" + str(responds_id))\n",
    "    else: \n",
    "        print('\\x1b[1;31m'+'Integrity of: ' + file_name + ' NOT confirmed, see errors' + '\\x1b[0m')\n",
    "    db.close()\n",
    "    return()\n",
    "\n",
    "\n",
    "\n",
    "def sent_certificate(organisation, file_name, username):\n",
    "    storage_id = random.randrange(1000) \n",
    "    args = [storage_id, file_name, organisation, username , time.strftime('%Y-%m-%d', time.localtime())]\n",
    "    URL = \"http://localhost:4000/channels/mychannel/chaincodes/integrityCertificate\"\n",
    "    PARAMS = {\"fcn\": \"createRecord\", \"chaincodeName\": \"integrityCertificate\", \"channelName\": \"mychannel\", \"args\" : args}\n",
    "    headers = {\"Authorization\": \"Bearer \" + t}\n",
    "    r = requests.post(url = URL, json = PARAMS, headers=headers)\n",
    "    return(storage_id)\n",
    "\n",
    "\n",
    "def verification_attributes(data, organisation, table_name):\n",
    "    batch_hash = [hashlib.sha256(repr(val).encode()).hexdigest() for val in data.T.apply(lambda x: (tuple(x)), axis = 1)]\n",
    "    h_v = hashlib.sha256(repr(\"\".join(batch_hash)).encode()).hexdigest()\n",
    "    file = {\n",
    "        'orgName': organisation,\n",
    "        'tableName': table_name, \n",
    "        'batch': int(data[\"metahistoryinsbatchid\"].unique()[0]),\n",
    "        'colN': len(data.columns),\n",
    "        'batchHash':h_v,\n",
    "        'timeStamp': data[\"metahistoryvalidfrom\"].unique()[0],\n",
    "        'hashes': batch_hash,\n",
    "        'cols:': list(data.columns) \n",
    "        }\n",
    "    return(file, h_v)\n",
    "\n",
    "#Compares the blockchain verification hash with the database verificaion hash.\n",
    "def verify_1(h_v, h_v_):\n",
    "    r = 0 \n",
    "    if(str(h_v) == h_v_):\n",
    "        k = 1 \n",
    "    else:\n",
    "        print(\"Integrity broken, investigate further\")\n",
    "        r = 1\n",
    "    return(r)\n",
    " \n",
    "    \n",
    "#Compares the distributed identification hashes with the database identification hashes.     \n",
    "def verify_2(h_i_, s_v, key): \n",
    "    file = get_from_ipfs(h_i_)\n",
    "    secret = get_key_from_blockchain(h_i_)\n",
    "    y = decrypt(key, secret, file)\n",
    "    r = 0\n",
    "    if(s_v['hashes'] == y['hashes'] ):\n",
    "        k = 1 \n",
    "    else:\n",
    "        pair_compare(s_v, y)\n",
    "        r = 1\n",
    "    return(r)\n",
    "  \n",
    "\n",
    "def get_table(table_name, db):            \n",
    "    return(pd.read_sql_query(\"SELECT * from \" + table_name, db))     \n",
    "\n",
    "def get_batch(data, batch_number):\n",
    "    return(data.loc[data['metahistoryinsbatchid'] == batch_number]) \n",
    "\n",
    "#Retrieve the identification and verification hash from the blockchain. \n",
    "def get_from_blockchain(organisation, table_name, batch_number ):\n",
    "    print(organisation)\n",
    "    print(table_name)\n",
    "    print(batch_number)\n",
    "    URL = \"http://localhost:5984/mychannel_$integrityVerification/_find\"\n",
    "    PARAMS = {\"selector\" : {\"Organisation\" : organisation, \"Table_name\" : table_name, \"Batch_ID\": str(batch_number)}}\n",
    "    r = requests.post(url = URL, json = PARAMS)\n",
    "    return(json.loads(r.text)['docs'][0][\"_id\"], json.loads(r.text)['docs'][0]['Verification_Hash']) \n",
    "\n",
    "def get_from_ipfs(h_i_):\n",
    "    return(client.cat(h_i_))\n",
    "\n",
    "def decrypt(key, nonce, file):\n",
    "    cipher = AES.new(bytes(key, 'utf-8'), AES.MODE_EAX, b64decode(bytes(nonce[3:],'ISO-8859-1')))\n",
    "    plaintext = cipher.decrypt(file)\n",
    "    return(json.loads(plaintext.decode(\"utf-8\")))\n",
    "\n",
    "def get_key_from_blockchain(h_i): \n",
    "    URL = \"http://localhost:4000/channels/mychannel/chaincodes/privateStorage\"\n",
    "    PARAMS = {\"fcn\": \"readPrivateKey\", \"args\" : \"\"\"[\"collectionPrivateDetails\" ,'\"\"\" +  h_i + \"']\"\"\"}\n",
    "    headers = {\"Authorization\": \"Bearer \" + t}\n",
    "    result = None\n",
    "    while result is None: \n",
    "        r = requests.get(url = URL, params = PARAMS, headers=headers)\n",
    "        try:\n",
    "            responds = json.loads(r.text)[\"result\"]['secrets']\n",
    "            return(responds)\n",
    "        except:\n",
    "            pass \n",
    "    return(responds) \n",
    "    \n",
    "\n",
    "#Compare the distributed verification hashes with the datawarehouse verification hashes. \n",
    "def pair_compare(s_v, y):\n",
    "    missing_col = list(set(s_v['cols:']) - set(y['cols:'])) + list(set(y['cols:']) - set(s_v['cols:'])) #search if there is a difference in columns. \n",
    "    if (len(missing_col) > 0): \n",
    "        print(\"Missing columns: \")\n",
    "        print(missing_col)\n",
    " \n",
    "    missing_hash = list(set(y['hashes']) - set(s_v['hashes'])) #search if there is a difference in hashes\n",
    "    print(missing_hash)\n",
    "    if (len(missing_hash) > 0): \n",
    "        print(\"Missing or wrong hash at column: \")\n",
    "        for hash_ in missing_hash:\n",
    "            n = y[\"hashes\"].index(hash_)\n",
    "            print(y['cols:'][n])\n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
