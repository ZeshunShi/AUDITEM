{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipfshttpclient'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cf6bab3ecd78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mipfshttpclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipfshttpclient'"
     ]
    }
   ],
   "source": [
    "import ipfshttpclient\n",
    "import requests\n",
    "import json \n",
    "import fiona \n",
    "import geopandas\n",
    "import pandas as pd\n",
    "client = ipfshttpclient.connect()\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import Crypto\n",
    "from Crypto.Cipher import AES\n",
    "from base64 import b64encode, b64decode\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_Name = \"SQlite-file\"\n",
    "username = \"Auditor\"\n",
    "organisation = \"Org1\"\n",
    "key = \"secret-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = add_user(username, organisation)\n",
    "tables_filtered = addDB(DB_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_database(DB_Name, tables_filtered, organisation, t, key )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To load the dataframe\n",
    "\n",
    "def get_tables(db):  #Get a list of the tables that are inside the database\n",
    "    cursor = db.cursor()\n",
    "    tables = list()\n",
    "    for name in cursor.execute('SELECT * FROM gpkg_contents;'):\n",
    "        tables.append(name[0])   \n",
    "    cursor.close()\n",
    "    return(tables)\n",
    "\n",
    "def filter_tables(db, tables): #Filter for usefull tables, return a list of the useful tables. \n",
    "    tables_filterd = list()\n",
    "    cursor = db.cursor()\n",
    "    for table in tables:\n",
    "        colNames = list() \n",
    "        for name in cursor.execute('PRAGMA table_info('+ table +');'):\n",
    "            colNames.append(name[1])\n",
    "        if \"metahistoryinsbatchid\" in colNames: #We only take tables that have the list metahistoryinsbatchid\n",
    "            tables_filterd.append(table)        \n",
    "    cursor.close() \n",
    "    return(tables_filterd) \n",
    "\n",
    "\n",
    "def get_amount_batches(tables_filtered, db):  #Get the numer of batched in the first col (BETTER TO LOOK AT AL DB\"s AND SELECT the most)\n",
    "    df = pd.read_sql_query(\"SELECT * from \" + tables_filtered[0], db)\n",
    "    return(df[\"metahistoryinsbatchid\"].unique())\n",
    "    \n",
    "\n",
    "def addDB(File_name): #Add DB To the system \n",
    "    db = sqlite3.connect(File_name)\n",
    "    tables = get_tables(db)\n",
    "    tables_filtered = filter_tables(db, tables)\n",
    "    print(\"There are: \" + str(len(tables_filtered)) + \" tables\")\n",
    "    print(\"Containing: \"+ str(len(get_amount_batches(tables_filtered, db))) + \" batches\")\n",
    "    db.close()\n",
    "    return(tables_filtered)\n",
    "\n",
    "#To add the dataframe\n",
    "\n",
    "def add_user(username, organization): #add user using the API in hyperledger, returns the token needed to add transaction tot the chain. \n",
    "    URL =\"http://localhost:4000/users\"\n",
    "    PARAMS = {'username':username, \n",
    "              'orgName': organization} \n",
    "    r = requests.post(url = URL, json = PARAMS)   \n",
    "    token = json.loads(r.text)['token'] \n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload complete database, input is the database name, what tables to upload and the transaction token. After this function the data is added to the blockchain and IPFS. \n",
    "def upload_database(file_name, tables_filtered, organisation, t, key):\n",
    "    db = sqlite3.connect(file_name)\n",
    "    for table_name in tables_filtered:\n",
    "        table = get_table(table_name, db)\n",
    "        for batch_number in table[\"metahistoryinsbatchid\"].unique():\n",
    "            subset = get_batch(table, batch_number)\n",
    "            upload_data(subset, organisation, table_name, t,key)\n",
    "    db.close()\n",
    "    return()\n",
    "\n",
    "#retrieve the table from the DB \n",
    "def get_table(table_name, db):            \n",
    "    return(pd.read_sql_query(\"SELECT * from \" + table_name, db))     \n",
    "\n",
    "#retrieve the specific batch from the DB \n",
    "def get_batch(data, batch_number):\n",
    "    return(data.loc[data['metahistoryinsbatchid'] == batch_number])         \n",
    "        \n",
    "#input is the single table batch, organisation, name of the table and token, output is the upload of the single batch to ipfs and blockchain. \n",
    "def upload_data(subset, organisation, table_name, t, key):\n",
    "    r = check_if_exists(organisation, table_name, int(subset[\"metahistoryinsbatchid\"].unique()[0]))\n",
    "    if len(r) > 0:   #is needed to check if the batch already is in the database, prevents reuploading. \n",
    "        print ('Batch already in Blockchain') \n",
    "    else:\n",
    "        s_v, h_v = verification_attributes(subset, organisation, table_name)\n",
    "        s_v_cyper, nonce  = encrypt(s_v, key)\n",
    "        h_i = sent_to_ipfs(s_v_cyper)\n",
    "        s_i = identification_attributes(h_i, organisation, table_name, subset, h_v)\n",
    "        sent_to_chain(s_i,t)\n",
    "        sent_key(h_i, nonce, key)\n",
    "    return()\n",
    "\n",
    "\n",
    "#Retrieve the attributes used for the integrity verification phase.    \n",
    "def verification_attributes(data, organisation, table_name):\n",
    "    batch_hash = [hashlib.sha256(repr(val).encode()).hexdigest() for val in data.T.apply(lambda x: (tuple(x)), axis = 1)]\n",
    "    h_v = hashlib.sha256(repr(\"\".join(batch_hash)).encode()).hexdigest()\n",
    "    file = {\n",
    "        'orgName': organisation,\n",
    "        'tableName': table_name, \n",
    "        'batch': int(data[\"metahistoryinsbatchid\"].unique()[0]),\n",
    "        'colN': len(data.columns),\n",
    "        'batchHash':h_v,\n",
    "        'timeStamp': data[\"metahistoryvalidfrom\"].unique()[0],\n",
    "        'hashes': batch_hash,\n",
    "        'cols:': list(data.columns) \n",
    "        }\n",
    "    return(file, h_v)\n",
    "\n",
    "#Retrieve the attributes used for the identification of the verification attributes. \n",
    "def identification_attributes(h_i, organisation, table_name, data, h_v):\n",
    "    args = [h_i, organisation, table_name, int(data[\"metahistoryinsbatchid\"].unique()[0]) , str(h_v)]\n",
    "    return(args)\n",
    "\n",
    "\n",
    "def encrypt(file,key):\n",
    "    cipher = AES.new(bytes(key, 'utf-8'), AES.MODE_EAX)\n",
    "    nonce = cipher.nonce\n",
    "    ciphertext, tag = cipher.encrypt_and_digest(json.dumps(file).encode('utf-8'))\n",
    "    return(ciphertext, b64encode(nonce))\n",
    "\n",
    "\n",
    "def sent_key (h_i, secret, key):\n",
    "    URL = \"http://localhost:4000/channels/mychannel/chaincodes/privateStorage\"\n",
    "    PARAMS = {\"fcn\": \"createPrivateKeyStorage\", \n",
    "              \"chaincodeName\": \"privateStorage\",\n",
    "              \"channelName\": \"mychannel\",\n",
    "              \"peers\": [\"peer0.org1.example.com\", \"peer0.org2.example.com\"],\n",
    "              \"args\" : [\"\"],\n",
    "              \"transient\": \"{\\\"keys\\\": {\\\"key\\\":\\\"\"+ str(h_i) +\"\\\",\\\"secretKey\\\":\\\"\"+key+\"\\\",\\\"nonce\\\":\\\" \"+ str(secret) +\"\\\"}} \"}\n",
    "    headers = {\"Authorization\": \"Bearer \" + t}\n",
    "    r = requests.post(url = URL, json = PARAMS, headers=headers)\n",
    "    return()\n",
    "    \n",
    "#Sending the verification attributes to the distributed file system, returns the identification hash. \n",
    "def sent_to_ipfs(file):\n",
    "    with open(\"file.txt\", \"w\") as f:\n",
    "        f.write(\"\")\n",
    "    with open(\"file.txt\", \"ab\") as f:\n",
    "        f.write(file)\n",
    "    res = client.add('file.txt')\n",
    "    return(res['Hash'])\n",
    "\n",
    "#sending the identification attributes with identification hash and verification hash to the blockchain. \n",
    "def sent_to_chain(args, t):\n",
    "    URL = \"http://localhost:4000/channels/mychannel/chaincodes/IntegrityVerification\"\n",
    "    PARAMS = {\"fcn\": \"createRecord\", \"chaincodeName\": \"IntegrityVerification\", \"channelName\": \"mychannel\", \"args\" : args}\n",
    "    headers = {\"Authorization\": \"Bearer \" + t}\n",
    "    r = requests.post(url = URL, json = PARAMS, headers=headers)\n",
    "    return()\n",
    "\n",
    "\n",
    "#Uses the identification attributes to check if the spefic batch is already added to the database. \n",
    "def check_if_exists(organisation, table_name, batch_number):\n",
    "    URL = \"http://localhost:5984/mychannel_$integrity_verification/_find\"\n",
    "    PARAMS = {\"selector\" : {\"Organisation\" : organisation, \"Table_name\" : table_name, \"Batch_ID\": str(batch_number)}}\n",
    "    r = requests.post(url = URL, json = PARAMS)\n",
    "    return(json.loads(r.text)['docs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
